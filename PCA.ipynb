{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of data generated each day from various sources such as scientific experiments, cell phones, smart watches, and other data products is increasing exponentially everyday. Certain machine learning tasks involve analyzing high-dimensional datasets (ie: datasets with a large number of features). Examples of these datasets could include: image data [ie: features could be image pixels] or recommender data [ie: where the features are a list of all movies rated by users]. We know that machine learning models can be used to classify or cluster these data in order to predict future events. Yet, datasets with large features pose a unique challenge for machine learning analysis. Datasets with a large number of features add complexity to certain machine learning models (ie: linear models), as a result, machine learning models that train on datasets with large features are more prone to producing error due to bias. Therefore, in order to reduce the likelihood of error, it might be helpful to compress the data and describe the data only using a few values before implementing supervised or unsupervised machine learning models.  \t\t Features within a large dataset may vary in a similar way. For example, consider data from a movie rating system where the movie ratings from different users are instances and the various movies are features (Figure 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Figure 1](https://raw.githubusercontent.com/lesleymaraina/PCA/master/Slide1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, you might observe that users who rank <i>Star Wars Episode IV </i>highly might also rank <i>Rogue One: A Star Wars Story </i>highly. In other words, the ratings for <i>Star Wars Episode IV</i> are positively correlated with <i>Rogue One: A Star Wars Story</i>. One could image that all movies ranked by certain users in a similar way might all share a similar attribute (ie: all movies with these rankings are classic sci-fi movies), and could ultimately be grouped to form a new feature (<b>Figure 2</b>).\n",
    "![Figure 2](https://raw.githubusercontent.com/lesleymaraina/PCA/master/Slide2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is an intuitive way of grouping data, but it would take quite some time to read through all of the data and group it according to similar attributes. Fortunately, there are algorithms that can automatically group features that vary in a similar way within high-dimensional datasets, these methods are called: dimensionality reduction algorithms. \n",
    "Principal component analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional datasets into a dataset with fewer variables, and the set of resulting variables explains the maximum variance within the dataset[<b>1</b>]. PCA is used prior to unsupervised and supervised machine learning steps to reduce the number of features used in the analysis[<b>2</b>]. The overall goal of PCA is to reduce the number of <b>d</b>-dimensions (ie: features) in a dataset by projecting it onto a <b>k</b>-dimensional subspace where <b>k < d </b> [<b>3</b>]. The approach used to complete PCA can be summarized as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tStandardize the data \n",
    "2.\tUse the standardized data to generate a covariance matrix (or perform Singular Vector Decomposition)\n",
    "3.\tObtain eigenvectors (principal components) and eigenvalues from the covariance matrix; each eigenvector will have a corresponding eigenvalue\n",
    "4.\tSort the eigenvalues in descending order\n",
    "5.\tSelect the <b>k</b> eigenvectors with the largest eigenvalues;  <b>k</b> is the number of dimensions used in the new feature space (<b>k≤d</b>)\n",
    "6.\tConstruct a new matrix with the selected <b>k</b> eigenvectors\n",
    "\n",
    "[<b>steps 1-6 source: Raschka(2105)</b>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of PCA analysis. We will use PCA to reduce the dimensions within the MovieLens movie ratings dataset. The data used in the following tutorial can be found [here](https://movielens.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll load the data from MovieLens.org, and store the data in a pandas dataframe. The data set contains ratings from 718 users for 8913 movies (features).  Even though all of the features in the dataset are measured on the same scale (ie: all ratings are on a scale (0-5)),  we must make sure that we standardize the data by transforming the data onto a unit scale (mean=0 and variance=1). Also, all NaN values were converted to 0. It is necessary to transform data because PCA can only be applied on numerical data[<b>4</b>]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import*\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "from scipy import stats\n",
    "#from wpca import PCA\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load movie names and movie ratings\n",
    "movies = pd.read_csv('https://raw.githubusercontent.com/lesleymaraina/PCA/master/movies.csv')\n",
    "ratings = pd.read_csv('https://raw.githubusercontent.com/lesleymaraina/PCA/master/ratings.csv')\n",
    "ratings.drop(['timestamp'], axis=1, inplace=True)\n",
    "\n",
    "def replace_name(x):\n",
    "\treturn movies[movies['movieId']==x].title.values[0]\n",
    "ratings.movieId = ratings.movieId.map(replace_name)\n",
    "\n",
    "M = ratings.pivot_table(index=['userId'], columns=['movieId'], values='rating')\n",
    "m = M.shape\n",
    "m\n",
    "\n",
    "df1 = M.replace(np.nan, 0, regex=True)\n",
    "X_std = StandardScaler().fit_transform(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Covariance Matrix and Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a covariance matrix is created based on the standardized data. The covariance matrix is a representation of the covariance between each feature in the original dataset3. The covariance matrix can be found as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix \n",
      "[[ 1.0013947  -0.00276421 -0.00195661 ..., -0.00858289 -0.00321221\n",
      "  -0.01055463]\n",
      " [-0.00276421  1.0013947  -0.00197311 ...,  0.14004611 -0.0032393\n",
      "  -0.01064364]\n",
      " [-0.00195661 -0.00197311  1.0013947  ..., -0.00612653 -0.0022929\n",
      "  -0.00753398]\n",
      " ..., \n",
      " [-0.00858289  0.14004611 -0.00612653 ...,  1.0013947   0.02888777\n",
      "   0.14005644]\n",
      " [-0.00321221 -0.0032393  -0.0022929  ...,  0.02888777  1.0013947\n",
      "   0.01676203]\n",
      " [-0.01055463 -0.01064364 -0.00753398 ...,  0.14005644  0.01676203\n",
      "   1.0013947 ]]\n",
      "NumPy covariance matrix: \n",
      "[[ 1.0013947  -0.00276421 -0.00195661 ..., -0.00858289 -0.00321221\n",
      "  -0.01055463]\n",
      " [-0.00276421  1.0013947  -0.00197311 ...,  0.14004611 -0.0032393\n",
      "  -0.01064364]\n",
      " [-0.00195661 -0.00197311  1.0013947  ..., -0.00612653 -0.0022929\n",
      "  -0.00753398]\n",
      " ..., \n",
      " [-0.00858289  0.14004611 -0.00612653 ...,  1.0013947   0.02888777\n",
      "   0.14005644]\n",
      " [-0.00321221 -0.0032393  -0.0022929  ...,  0.02888777  1.0013947\n",
      "   0.01676203]\n",
      " [-0.01055463 -0.01064364 -0.00753398 ...,  0.14005644  0.01676203\n",
      "   1.0013947 ]]\n"
     ]
    }
   ],
   "source": [
    "#Create a covariance matrix\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "print('Covariance matrix \\n%s' %cov_mat)\n",
    "\n",
    "#Create the same covariance matrix with 1 line of code\n",
    "print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the covariance matrix is generated, eigendecomposition is performed on the covariance matrix. Eigenvectors and eigenvalues are found as a result of the eigendceomposition. Each eigenvector has a corresponding eigenvalue, and the sum of the eigenvalues represents all of the variance within the entire dataset. The eigendecomposition can be completed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform eigendecomposition on covariance matrix\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Selecting Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors or principal components are a normalized linear combination of the features in the original dataset4. The first principal component captures the most variance in the original variables, and the second component is a representation of the second highest variance within the dataset. For example, if you were to plot data from a dataset that contains 2 features, the following illustrates that principal component 1 (PC1) represents the direction of the most variation between the 2 features and PC2 represents the second most variation between the 2 plotted features (<b>Figure 3</b>). Our movies data contains over 8000 features and would be difficult to visualize which is why we used eigendecomposition to generate the eigenvectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 3](https://raw.githubusercontent.com/lesleymaraina/PCA/master/Slide3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors with the lowest eigenvalues describe the least amount of variation within the dataset. Therefore, these values can be dropped. First, lets order the eigenvalues in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of how principal components describe the variance in the data, we will look at the explained variance ratio of the first 2 principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(df1)\n",
    "print pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 principal components describe ~14% of the variance in the data. In order gain a more comprehensive idea on how each principal component explains the variance within the data, we will construct a scree plot. A scree plot displays the variance explained by each principal component within the analysis[<b>5</b>]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explained variance\n",
    "pca = PCA().fit(X_std)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scree plot shows that the first 480 principal components describe most of the variation (information) within the data. This is a major reduction from the initial 8913 features. Therefore, the first 480 eigenvectors should be used to construct the dimensions for the new feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References:\n",
    "1.\thttp://scikit-learn.org/stable/modules/decomposition.html  \n",
    "2.\thttp://scikit-learn.org/stable/modules/unsupervised_reduction.html\n",
    "3.\tRaschka, Sebastian. Principle Component Analysis in 3 Simple Steps, (2015). http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html \n",
    "4.\thttp://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/ \n",
    "5.\thttp://support.minitab.com/en-us/minitab/17/topic-library/modeling-statistics/multivariate/principal-components-and-factor-analysis/what-is-a-scree-plot/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
